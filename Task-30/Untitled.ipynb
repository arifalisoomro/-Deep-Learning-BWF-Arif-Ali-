{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32e33e3f",
   "metadata": {},
   "source": [
    "### RNN\n",
    "Recurrent Neural Networks (RNNs) are a class of neural networks designed to process sequential data, such as time series or text. They have the ability to maintain a hidden state that captures information about previous inputs, making them suitable for tasks involving sequential dependencies. Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) are two popular types of RNN architectures that address the vanishing gradient problem and improve the model's ability to capture long-term dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34437fb",
   "metadata": {},
   "source": [
    "### Long Short-Term Memory (LSTM):\n",
    "LSTM is an RNN architecture that introduces memory cells and gating mechanisms to control the flow of information within the network. The key components of an LSTM cell are:\n",
    "\n",
    "Cell State (Ct): It represents the memory or information that flows through the entire sequence.\n",
    "Forget Gate (ft): It decides which information to discard from the cell state.\n",
    "Input Gate (it): It determines which new information to store in the cell state.\n",
    "Output Gate (ot): It controls the output of the LSTM cell based on the current input and the cell state.\n",
    "The LSTM architecture enables the model to capture and remember long-term dependencies in sequential data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f340b2",
   "metadata": {},
   "source": [
    "### Gated Recurrent Unit (GRU):\n",
    "GRU is another RNN architecture that simplifies the LSTM by combining the cell state and hidden state into a single vector. The key components of a GRU cell are:\n",
    "\n",
    "Update Gate (zt): It decides how much of the previous hidden state to consider for the current state.\n",
    "Reset Gate (rt): It determines how much of the previous hidden state to forget.\n",
    "Current Memory (ht): It represents the new hidden state or the updated memory.\n",
    "GRU simplifies the LSTM architecture by removing the separate cell state, which can make it computationally more efficient in some cases.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdc6940",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.LSTM(128, input_shape=(timesteps, input_dim)))\n",
    "model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
