{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6396f7d5",
   "metadata": {},
   "source": [
    "### One-Hot Encoding:\n",
    "One-Hot Encoding is used to represent categorical variables as binary vectors. Each category is represented by a binary vector where all elements are zero except for the index corresponding to the category, which is set to one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2d7d9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   color  blue  green  red\n",
      "0    red     0      0    1\n",
      "1   blue     1      0    0\n",
      "2  green     0      1    0\n",
      "3    red     0      0    1\n",
      "4  green     0      1    0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame with a categorical variable\n",
    "data = {'color': ['red', 'blue', 'green', 'red', 'green']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Perform one-hot encoding\n",
    "one_hot_encoded = pd.get_dummies(df['color'])\n",
    "\n",
    "# Concatenate the encoded columns with the original DataFrame\n",
    "df_encoded = pd.concat([df, one_hot_encoded], axis=1)\n",
    "print(df_encoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71aef497",
   "metadata": {},
   "source": [
    "### Word Embeddings:\n",
    "Word Embeddings are dense vector representations that capture semantic relationships between words. They are widely used in Natural Language Processing (NLP) tasks. Popular algorithms for generating word embeddings include Word2Vec, GloVe, and FastText. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90af5e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8.1681199e-03 -4.4430327e-03  8.9854337e-03  8.2536647e-03\n",
      " -4.4352221e-03  3.0310510e-04  4.2744912e-03 -3.9263200e-03\n",
      " -5.5599655e-03 -6.5123225e-03 -6.7073823e-04 -2.9592158e-04\n",
      "  4.4630850e-03 -2.4740540e-03 -1.7260908e-04  2.4618758e-03\n",
      "  4.8675989e-03 -3.0808449e-05 -6.3394094e-03 -9.2608072e-03\n",
      "  2.6657581e-05  6.6618943e-03  1.4660227e-03 -8.9665223e-03\n",
      " -7.9386048e-03  6.5519023e-03 -3.7856805e-03  6.2549924e-03\n",
      " -6.6810320e-03  8.4796622e-03 -6.5163244e-03  3.2880199e-03\n",
      " -1.0569858e-03 -6.7875278e-03 -3.2875966e-03 -1.1614120e-03\n",
      " -5.4709399e-03 -1.2113475e-03 -7.5633135e-03  2.6466595e-03\n",
      "  9.0701487e-03 -2.3772502e-03 -9.7651005e-04  3.5135616e-03\n",
      "  8.6650876e-03 -5.9218528e-03 -6.8875779e-03 -2.9329848e-03\n",
      "  9.1476962e-03  8.6626766e-04 -8.6784009e-03 -1.4469790e-03\n",
      "  9.4794659e-03 -7.5494875e-03 -5.3580985e-03  9.3165627e-03\n",
      " -8.9737261e-03  3.8259076e-03  6.6544057e-04  6.6607012e-03\n",
      "  8.3127534e-03 -2.8507852e-03 -3.9923131e-03  8.8979173e-03\n",
      "  2.0896459e-03  6.2489416e-03 -9.4457148e-03  9.5901238e-03\n",
      " -1.3483083e-03 -6.0521150e-03  2.9925345e-03 -4.5661093e-04\n",
      "  4.7064926e-03 -2.2830211e-03 -4.1378425e-03  2.2778988e-03\n",
      "  8.3543835e-03 -4.9956059e-03  2.6686788e-03 -7.9905549e-03\n",
      " -6.7733466e-03 -4.6766878e-04 -8.7677278e-03  2.7894378e-03\n",
      "  1.5985954e-03 -2.3196924e-03  5.0037908e-03  9.7487867e-03\n",
      "  8.4542679e-03 -1.8802249e-03  2.0581519e-03 -4.0036892e-03\n",
      " -8.2414057e-03  6.2779556e-03 -1.9491815e-03 -6.6620467e-04\n",
      " -1.7713320e-03 -4.5356657e-03  4.0617096e-03 -4.2701806e-03]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Example sentences\n",
    "sentences = [['I', 'love', 'machine', 'learning'],\n",
    "             ['Machine', 'learning', 'is', 'fun'],\n",
    "             ['I', 'enjoy', 'coding']]\n",
    "\n",
    "# Train the Word2Vec model\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "\n",
    "# Get the word embeddings\n",
    "word_embeddings = model.wv\n",
    "\n",
    "# Get the embedding vector for a word\n",
    "embedding_vector = word_embeddings['machine']\n",
    "print(embedding_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8132d3cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
